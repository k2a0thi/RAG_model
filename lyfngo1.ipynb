{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4  transformers\n",
        "import sqlite3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEK6tbThuB60",
        "outputId": "a6d2a588-fa44-433a-ef28-e962b77e2946"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import sqlite3\n",
        "from transformers import pipeline\n",
        "\n",
        "# Function to scrape Wikipedia data\n",
        "def scrape_wikipedia(url):\n",
        "    try:\n",
        "        # Send an HTTP request to the Wikipedia page\n",
        "        response = requests.get(url)\n",
        "\n",
        "        # Check if the request was successful\n",
        "        if response.status_code == 200:\n",
        "            # Parse the page content using BeautifulSoup\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            # Extract the page title\n",
        "            title = soup.find('h1', {'id': 'firstHeading'}).text\n",
        "\n",
        "            # Extract the main content of the page (bodyContent div contains the entire page content)\n",
        "            content_section = soup.find('div', {'id': 'bodyContent'})\n",
        "\n",
        "            # Initialize an empty list to store the content\n",
        "            full_content = []\n",
        "\n",
        "            # Iterate over the content in the main section\n",
        "            for element in content_section.find_all(['h1', 'h2', 'h3', 'p', 'ul', 'ol']):\n",
        "                if element.name.startswith('h'):\n",
        "                    # For headings (h1, h2, h3), append the text as a section header\n",
        "                    full_content.append(f\"\\n{element.text.strip()}\\n{'=' * len(element.text)}\\n\")\n",
        "                elif element.name == 'p':\n",
        "                    # For paragraphs, append the paragraph text\n",
        "                    full_content.append(element.text.strip())\n",
        "                elif element.name in ['ul', 'ol']:\n",
        "                    # For lists (ul, ol), extract each list item and append\n",
        "                    for li in element.find_all('li'):\n",
        "                        full_content.append(f\" - {li.text.strip()}\")\n",
        "\n",
        "            # Combine all content into a single string\n",
        "            full_text = \"\\n\".join(full_content)\n",
        "\n",
        "            return {\"title\": title, \"content\": full_text}\n",
        "        else:\n",
        "            return {\"error\": f\"Failed to retrieve page. Status code: {response.status_code}\"}\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"An error occurred: {str(e)}\"}\n",
        "\n",
        "# Function to create an SQLite database and insert data\n",
        "def setup_database():\n",
        "    # Remove the existing database file if it exists\n",
        "    if os.path.exists('wikipedia_data.db'):\n",
        "        os.remove('wikipedia_data.db')\n",
        "\n",
        "    # Connect to SQLite (or create it if it doesn't exist)\n",
        "    conn = sqlite3.connect('wikipedia_data.db')\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Create a table to store Wikipedia data\n",
        "    cursor.execute('''CREATE TABLE IF NOT EXISTS WikipediaPages (\n",
        "                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                        title TEXT NOT NULL,\n",
        "                        content TEXT NOT NULL\n",
        "                    )''')\n",
        "    conn.commit()\n",
        "    return conn, cursor\n",
        "\n",
        "# Function to insert scraped data into the SQLite database\n",
        "def insert_data_into_db(cursor, title, content):\n",
        "    cursor.execute('''INSERT INTO WikipediaPages (title, content) VALUES (?, ?)''', (title, content))\n",
        "\n",
        "# Function to retrieve content from the database\n",
        "def get_content_from_db(cursor):\n",
        "    cursor.execute('SELECT content FROM WikipediaPages ORDER BY id DESC LIMIT 1')\n",
        "    result = cursor.fetchone()\n",
        "    return result[0] if result else None\n",
        "\n",
        "# Function to delete all content from the database\n",
        "def delete_content_from_db(cursor):\n",
        "    cursor.execute('DELETE FROM WikipediaPages')\n",
        "    print(\"Content deleted from database.\")\n",
        "\n",
        "# Main loop for question answering\n",
        "def question_answer_loop(cursor):\n",
        "    # Load the question-answering model\n",
        "    qa_model = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
        "\n",
        "    while True:\n",
        "        # Get question from user\n",
        "        question = input(\"Ask a question (or type 'quit' to exit): \").strip()\n",
        "\n",
        "        # Check if user wants to quit\n",
        "        if question.lower() == \"quit\":\n",
        "            delete_content_from_db(cursor)\n",
        "            break\n",
        "\n",
        "        # Get content from the database\n",
        "        content = get_content_from_db(cursor)\n",
        "\n",
        "        if content:\n",
        "            # Perform question answering\n",
        "            result = qa_model(question=question, context=content)\n",
        "            answer = result['answer']\n",
        "            # Truncate the answer to two lines\n",
        "            truncated_answer = '\\n'.join(answer.split('\\n')[:2])\n",
        "            print(f\"Answer: {truncated_answer}\")\n",
        "        else:\n",
        "            print(\"No content found in the database.\")\n",
        "\n",
        "# Input: Wikipedia URL\n",
        "wikipedia_url = input(\"Enter a Wikipedia URL: \")\n",
        "scraped_data = scrape_wikipedia(wikipedia_url)\n",
        "\n",
        "# Output the scraped data and store it in the database\n",
        "if 'error' in scraped_data:\n",
        "    print(scraped_data['error'])\n",
        "else:\n",
        "    # Set up the database\n",
        "    conn, cursor = setup_database()\n",
        "\n",
        "    # Insert data into the database\n",
        "    insert_data_into_db(cursor, scraped_data['title'], scraped_data['content'])\n",
        "\n",
        "    # Commit the changes\n",
        "    conn.commit()\n",
        "\n",
        "    # Start the question answering loop\n",
        "    question_answer_loop(cursor)\n",
        "\n",
        "    # Close the database connection\n",
        "    conn.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwzP2SkCFuHh",
        "outputId": "566c2365-ce8a-46a8-8099-92a67a2c9d74"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a Wikipedia URL: https://en.wikipedia.org/wiki/Social_media\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ask a question (or type 'quit' to exit): what is social media?\n",
            "Answer: Viral Advertising\n",
            "Ask a question (or type 'quit' to exit): quit\n",
            "Content deleted from database.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn pyngrok nest_asyncio pydantic transformers beautifulsoup4 requests\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWlDetoc-EDs",
        "outputId": "8a253aea-e9b0-4e6b-d610-48eba626b7c6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastapi\n",
            "  Downloading fastapi-0.115.2-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.31.1-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (2.9.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Collecting starlette<0.41.0,>=0.37.2 (from fastapi)\n",
            "  Downloading starlette-0.40.0-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic) (2.23.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.41.0,>=0.37.2->fastapi) (3.7.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.41.0,>=0.37.2->fastapi) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.41.0,>=0.37.2->fastapi) (1.2.2)\n",
            "Downloading fastapi-0.115.2-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.7/94.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.31.1-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyngrok-7.2.0-py3-none-any.whl (22 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.40.0-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyngrok, h11, uvicorn, starlette, fastapi\n",
            "Successfully installed fastapi-0.115.2 h11-0.14.0 pyngrok-7.2.0 starlette-0.40.0 uvicorn-0.31.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from bs4 import BeautifulSoup\n",
        "import sqlite3\n",
        "from transformers import pipeline\n",
        "import uvicorn\n",
        "from pyngrok import ngrok\n",
        "import nest_asyncio\n",
        "import logging\n",
        "import threading\n",
        "\n",
        "# Allow nested async calls in Google Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Initialize FastAPI app\n",
        "app = FastAPI()\n",
        "\n",
        "# Function to scrape Wikipedia data\n",
        "def scrape_wikipedia(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            title = soup.find('h1', {'id': 'firstHeading'}).text\n",
        "            content_section = soup.find('div', {'id': 'bodyContent'})\n",
        "            full_content = []\n",
        "\n",
        "            for element in content_section.find_all(['h1', 'h2', 'h3', 'p', 'ul', 'ol']):\n",
        "                if element.name.startswith('h'):\n",
        "                    full_content.append(f\"\\n{element.text.strip()}\\n{'=' * len(element.text)}\\n\")\n",
        "                elif element.name == 'p':\n",
        "                    full_content.append(element.text.strip())\n",
        "                elif element.name in ['ul', 'ol']:\n",
        "                    for li in element.find_all('li'):\n",
        "                        full_content.append(f\" - {li.text.strip()}\")\n",
        "\n",
        "            full_text = \"\\n\".join(full_content)\n",
        "            return {\"title\": title, \"content\": full_text}\n",
        "        else:\n",
        "            logger.error(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
        "            return {\"error\": f\"Failed to retrieve page. Status code: {response.status_code}\"}\n",
        "    except Exception as e:\n",
        "        logger.exception(\"An error occurred while scraping Wikipedia.\")\n",
        "        return {\"error\": f\"An error occurred: {str(e)}\"}\n",
        "\n",
        "# Function to create an SQLite database and insert data\n",
        "def setup_database():\n",
        "    if os.path.exists('wikipedia_data.db'):\n",
        "        os.remove('wikipedia_data.db')\n",
        "    conn = sqlite3.connect('wikipedia_data.db')\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute('''CREATE TABLE IF NOT EXISTS WikipediaPages (\n",
        "                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                        title TEXT NOT NULL,\n",
        "                        content TEXT NOT NULL\n",
        "                    )''')\n",
        "    conn.commit()\n",
        "    return conn, cursor\n",
        "\n",
        "# Function to insert scraped data into the SQLite database\n",
        "def insert_data_into_db(cursor, title, content):\n",
        "    cursor.execute('''INSERT INTO WikipediaPages (title, content) VALUES (?, ?)''', (title, content))\n",
        "\n",
        "# Pydantic models for request bodies\n",
        "class LoadDataRequest(BaseModel):\n",
        "    url: str\n",
        "\n",
        "# Endpoint to load data from Wikipedia URL\n",
        "@app.post(\"/load\")\n",
        "async def load_data(request: LoadDataRequest):\n",
        "    scraped_data = scrape_wikipedia(request.url)\n",
        "    if 'error' in scraped_data:\n",
        "        raise HTTPException(status_code=400, detail=scraped_data['error'])\n",
        "\n",
        "    conn, cursor = setup_database()\n",
        "    insert_data_into_db(cursor, scraped_data['title'], scraped_data['content'])\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "    logger.info(f\"Data loaded successfully for page: {scraped_data['title']}\")\n",
        "    return {\"message\": f\"Data loaded successfully for page: {scraped_data['title']}\"}\n",
        "\n",
        "# Function to run the FastAPI server\n",
        "def run_fastapi():\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "# Function to start ngrok tunnel\n",
        "def start_ngrok():\n",
        "    logger.info(\"Starting ngrok...\")\n",
        "    public_url = ngrok.connect(8000)\n",
        "    logger.info(f\"Ngrok tunnel \\\"{public_url}\\\" is live!\")\n",
        "    return public_url\n",
        "\n",
        "# Main function to run FastAPI and ngrok in the background\n",
        "def run_fastapi_and_ngrok():\n",
        "    logger.info(\"Starting FastAPI server in a new thread...\")\n",
        "\n",
        "    # Start FastAPI server in a separate thread\n",
        "    fastapi_thread = threading.Thread(target=run_fastapi)\n",
        "    fastapi_thread.start()\n",
        "\n",
        "    # Start ngrok\n",
        "    public_url = start_ngrok()\n",
        "\n",
        "    return public_url\n",
        "\n",
        "# Run the FastAPI server and ngrok\n",
        "public_url = run_fastapi_and_ngrok()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hGqzQ-ejNF7",
        "outputId": "13b74ed9-758a-48fa-febb-4d932452190d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-10' coro=<Server.serve() done, defined at /usr/local/lib/python3.10/dist-packages/uvicorn/server.py:67> exception=KeyboardInterrupt()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/main.py\", line 579, in run\n",
            "    server.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/server.py\", line 65, in run\n",
            "    return asyncio.run(self.serve(sockets=sockets))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 30, in run\n",
            "    return loop.run_until_complete(task)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\", line 133, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 315, in __wakeup\n",
            "    self.__step()\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/server.py\", line 68, in serve\n",
            "    with self.capture_signals():\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 142, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/server.py\", line 328, in capture_signals\n",
            "    signal.raise_signal(captured_signal)\n",
            "KeyboardInterrupt\n",
            "INFO:     Started server process [270]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        }
      ]
    }
  ]
}